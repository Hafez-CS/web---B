# app.py - Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ù†ØªØ®Ø§Ø¨ "Ø­Ø§Ù„Øª Ø§Ù†ØªØ®Ø§Ø¨ Ù‡Ø¯Ù" Ùˆ DeepSeek AI

import os
import io
import json
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import pandas as pd
import httpx # ğŸ‘ˆ Ø¬Ø¯ÛŒØ¯: Ø¨Ø±Ø§ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API
from dotenv import load_dotenv # ğŸ‘ˆ Ø¬Ø¯ÛŒØ¯: Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ

from fastapi import FastAPI, UploadFile, File, HTTPException, Form 
from fastapi.responses import JSONResponse
from pydantic import BaseModel 

from anyio import to_thread

# Sklearn Imports
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Algorithm Imports (XGBoost and LightGBM are optional)
try:
    from xgboost import XGBRegressor
except ImportError:
    XGBRegressor = None
try:
    from lightgbm import LGBMRegressor
except ImportError:
    LGBMRegressor = None

# Allowed Algorithms
ALLOWED_REGRESSORS = ["LinearRegression", "RandomForestRegressor"]
if XGBRegressor:
    ALLOWED_REGRESSORS.append("XGBRegressor")
if LGBMRegressor:
    ALLOWED_REGRESSORS.append("LGBMRegressor")


# ===================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·ÛŒ Ùˆ AI =====================

load_dotenv() # ğŸ‘ˆ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ø§Ø² .env

AI_PROVIDER = os.getenv("AI_PROVIDER")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://openrouter.ai/api/v1")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek/deepseek-chat")


# ===================== FastAPI Setup =====================

app = FastAPI(
    title="Smart Financial Forecasting Platform (Unified API)",
    description=("Supports Regression Models with AI Target Selection."),
    version="2.4.0", # Ù†Ø³Ø®Ù‡ Ø¨Ù‡ Ø±ÙˆØ² Ø´Ø¯Ù‡ - Ø¨Ø¯ÙˆÙ† plotting
)


class FullAnalysisRequest(BaseModel):
    test_size: float = 0.2
    random_state: int = 42
    max_hist: int = 6
    max_rows_preview: int = 5


# ===================== ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ =====================

def basic_clean(df: pd.DataFrame) -> pd.DataFrame:
    df = df.dropna(axis=1, how="all").copy()
    for col in df.columns:
        if df[col].dtype == "object":
            try:
                coerced = pd.to_numeric(df[col].astype(str).str.replace(",", ""), errors="coerce")
                if coerced.notna().mean() > 0.5:
                    df[col] = coerced
            except Exception:
                pass
    return df

def split_cols(df: pd.DataFrame) -> Tuple[List[str], List[str]]:
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    cat_cols = [c for c in df.columns if c not in num_cols and df[c].nunique() > 1]
    return num_cols, cat_cols

def make_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    num_cols, cat_cols = split_cols(X)
    num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")),("scaler", StandardScaler())])
    cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")),("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))]) if cat_cols else "drop"
    return ColumnTransformer(transformers=[("num", num_pipe, num_cols),("cat", cat_pipe, cat_cols)],remainder="drop")

# ØªØ§Ø¨Ø¹ Fallback Ø¯Ø§Ø®Ù„ÛŒ (Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ AI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
def fallback_algo_and_target(df: pd.DataFrame) -> Dict[str, Any]:
    cols = df.columns.tolist()
    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    
    if "RandomForestRegressor" in ALLOWED_REGRESSORS: algo = "RandomForestRegressor"
    elif "XGBRegressor" in ALLOWED_REGRESSORS: algo = "XGBRegressor"
    else: algo = "LinearRegression"
    
    cand_names = ["profit","net_profit","revenue","income","sales","turnover","cost","expense","net_income", "charges", "price", "value", "ph", "quality"] 
    lower_cols = [c.lower() for c in cols]
    probable_target = None
    for nm in cand_names:
        if nm in lower_cols:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† Ø§ØµÙ„ÛŒ Ú©Ù‡ Ù†Ø§Ù… Ú©ÙˆÚ†Ú©â€ŒØ´Ø¯Ù‡ Ø¢Ù† ØªØ·Ø§Ø¨Ù‚ Ø¯Ø§Ø±Ø¯
            probable_target = cols[lower_cols.index(nm)] 
            break
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø³ØªÙˆÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³Øª
    if probable_target is None or probable_target not in num_cols: 
        probable_target = num_cols[-1] if num_cols else None
    
    return {"algorithm": algo, "target_column": probable_target}

# ===================== ØªØ§Ø¨Ø¹ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ DeepSeek AI ğŸ‘ˆ Ø¬Ø¯ÛŒØ¯ =====================

async def get_ai_suggestions_from_deepseek(df: pd.DataFrame, allowed_regressors: List[str]) -> Dict[str, Any]:
    """
    Ø§Ø² DeepSeek API Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ùˆ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ø¨Ù‡ ØªØ§Ø¨Ø¹ fallback Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.
    """
    if AI_PROVIDER != "deepseek" or not DEEPSEEK_API_KEY:
        print("DeepSeek API not configured or API key missing. Falling back to internal logic.")
        return fallback_algo_and_target(df) 
    
    # ØªÙ‡ÛŒÙ‡ Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø±Ø§ÛŒ DeepSeek
    # Ù†Ù…Ø§ÛŒØ´ ÛŒÚ© Ø³Ø·Ø± Ø§Ø² Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… (Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)
    column_info = df.head(1).T.to_dict().get(0, {}) 
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    
    prompt_message = f"""
    You are an AI expert in financial data analysis and machine learning model selection.
    Your task is to select the BEST target column (for regression) and a suitable algorithm 
    from the allowed list for a predictive model based on the provided CSV data.
    
    1. The target column MUST be **numeric** and present in the 'Available Numeric Columns for Target' list.
    2. The selected algorithm MUST be from the 'Allowed Algorithms' list.
    3. Prioritize columns related to financial outcomes (e.g., 'profit', 'revenue', 'price', 'value') 
       or measurable outcomes ('quality', 'ph') as the target.
    
    Available Columns (Type/Sample Value - First Row):
    {column_info}
    
    Available Numeric Columns for Target:
    {numeric_cols}
    
    Allowed Algorithms (Select ONE):
    {allowed_regressors}
    
    Output MUST be a single JSON object (with no other text/markdown or explanation) like this:
    {{"target_column": "SelectedColumnName", "algorithm": "SelectedAlgorithmName"}}
    """

    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": DEEPSEEK_MODEL,
        "messages": [
            {"role": "system", "content": "You are an expert ML model selector who outputs only a single JSON object."},
            {"role": "user", "content": prompt_message}
        ],
        "temperature": 0.1,
        "max_tokens": 150 # ÙØ¶Ø§ÛŒ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® JSON
    }

    try:
        async with httpx.AsyncClient(base_url=DEEPSEEK_BASE_URL) as client:
            print(f"Calling DeepSeek at {DEEPSEEK_BASE_URL}/chat/completions with model {DEEPSEEK_MODEL}")
            response = await client.post("/chat/completions", headers=headers, json=payload, timeout=30.0)
            response.raise_for_status() 
            
            response_json = response.json()
            raw_content = response_json['choices'][0]['message']['content'].strip()

            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ (Ø§Ú¯Ø± Ø¯Ø± Markdown JSON Ù…Ø­ØµÙˆØ± Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯)
            if raw_content.startswith("```json"):
                raw_content = raw_content.strip("```json").strip("```").strip()
                
            ai_choice = json.loads(raw_content)
            
            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø®Ø±ÙˆØ¬ÛŒ AI
            target = ai_choice.get("target_column")
            algo = ai_choice.get("algorithm")

            if target in df.columns and algo in allowed_regressors:
                print(f"DeepSeek selected target: {target}, algorithm: {algo}")
                return {"target_column": target, "algorithm": algo}
            else:
                print(f"DeepSeek output invalid or column/algo not found. Falling back. Target: {target}, Algo: {algo}")
                return fallback_algo_and_target(df)
            
    except Exception as e:
        print(f"Error calling DeepSeek API: {type(e).__name__}: {e}. Falling back to internal logic.")
        return fallback_algo_and_target(df)

# ===================== Prediction Logic (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) =====================
def run_prediction(df: pd.DataFrame, target: str, algo: str, input_filename: str, test_size: float = 0.2, random_state: int = 42) -> Dict[str, Any]:
    """Runs the ML model, generates files, and extracts prediction data."""
    
    print(f"--- START: Running prediction for target: {target} with {algo}. Data size: {df.shape}") 

    if target not in df.columns: raise ValueError(f"Target column '{target}' not found in data.")
    if not pd.api.types.is_numeric_dtype(df[target]): df[target] = pd.to_numeric(df[target], errors="coerce")
    clean_df = df.dropna(subset=[target])
    if clean_df.empty or clean_df.shape[0] < 5: raise ValueError("Data is too small or target column is empty after cleaning.")

    feature_cols = [c for c in clean_df.columns if c != target]
    if not feature_cols: raise ValueError("Data contains only the target column after cleaning. Cannot train a model.")
        
    X = clean_df[feature_cols] 
    y = clean_df[target]
    pre = make_preprocessor(X)
    
    # Model Selection
    if algo == "XGBRegressor" and XGBRegressor:
        model_instance = XGBRegressor(n_estimators=300, random_state=random_state, n_jobs=-1)
    elif algo == "LGBMRegressor" and LGBMRegressor:
        model_instance = LGBMRegressor(n_estimators=300, random_state=random_state, n_jobs=-1)
    elif algo == "RandomForestRegressor":
        model_instance = RandomForestRegressor(n_estimators=300, random_state=random_state, n_jobs=-1)
    elif algo == "LinearRegression":
        model_instance = LinearRegression()
    else:
        # Fallback to default if selected algo is unavailable
        algo = "LinearRegression"
        model_instance = LinearRegression()


    pipe = Pipeline([("preprocess", pre), ("model", model_instance)])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=False)
    
    pipe.fit(X_train, y_train) 
    print("--- MID: Model training completed.") 

    # --- Metrics Calculation ---
    if X_test.empty:
        y_pred_test = np.array([])
        mae = float('nan') 
        rmse = float('nan') 
    else:
        y_pred_test = pipe.predict(X_test)
        mae = float(mean_absolute_error(y_test, y_pred_test))
        rmse = float(np.sqrt(mean_squared_error(y_test, y_pred_test)))
    # --- End Metrics Calculation ---

    # Future Prediction
    FUTURE_STEPS = 5 
    if X_test.shape[0] > 0:
        last_X_test_row = X_test.iloc[[X_test.shape[0] - 1]]
        X_future = pd.concat([last_X_test_row] * FUTURE_STEPS, ignore_index=True)
        y_future_pred = pipe.predict(X_future)
    else:
        y_future_pred = np.array([])
    y_future_pred_list = y_future_pred.tolist()
    
    # Combine raw data
    y_actual_full = y_train.tolist() + y_test.tolist()
    y_pred_train = pipe.predict(X_train)
    y_pred_full = y_pred_train.tolist() + y_pred_test.tolist()
    
    # Apply Smoothing (Moving Average)
    SMOOTHING_WINDOW = max(5, int(len(y_actual_full) * 0.01)) 
    y_actual_series = pd.Series(y_actual_full)
    y_pred_series = pd.Series(y_pred_full)
    y_actual_smooth = y_actual_series.rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean().tolist()
    y_pred_smooth = y_pred_series.rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean().tolist()

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø¨Ø±Ø§ÛŒ response
    raw_prediction_data = {
        "actual_values": y_actual_smooth, 
        "predicted_values": y_pred_smooth,
        "future_predictions": y_future_pred_list,
        "smoothing_window": SMOOTHING_WINDOW
    }
        
    print("--- END: Prediction function finished successfully.")
        
    return {
        "algorithm_used": algo,
        "target_used": target,
        "metrics": {"MAE": mae, "RMSE": rmse},
        "prediction_plot_data": raw_prediction_data,
        "prediction_status": "Success"
    }


# ===================== Unified Endpoint (POST) ğŸ‘ˆ ØªØºÛŒÛŒØ±Ø§Øª Ø§ØµÙ„ÛŒ Ø¯Ø± Ù…Ù†Ø·Ù‚ AI =====================
@app.post("/full_analysis")
async def full_analysis(
    file: UploadFile = File(..., description="The CSV file containing financial data."),
    config: str = Form(
        '{"test_size":0.2,"random_state":42,"max_hist":6,"max_rows_preview":5}',
        description="Configuration JSON as a string (e.g., '{\"test_size\":0.2, ...}')."
    ),
    selection_mode: str = Form(..., description="Selection mode: 'ai' for automatic selection or 'user' for manual."),
    target_column: Optional[str] = Form(None, description="The specific target column name if selection_mode is 'user'."),
    user_id: Optional[str] = Form(None, description="The user ID for tracking purposes."),
    room_id: Optional[str] = Form(None, description="The room ID for tracking purposes.")
):
    
    all_columns = []
    target_to_use = "N/A" 
    algo_to_use = "RandomForestRegressor" # Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø§Ø®Ù„ÛŒ
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØµØ­Øª selection_mode
    if selection_mode not in ["ai", "user"]:
        raise HTTPException(status_code=400, detail="selection_mode must be 'ai' or 'user'.")
        
    try:
        config_dict = json.loads(config)
        config_model = FullAnalysisRequest(**config_dict)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid config format: {e}")
        
    try:
        input_filename = file.filename
        if not input_filename: input_filename = "uploaded_data.csv"
        
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content)) 
        df = basic_clean(df)
        
        all_columns = df.columns.tolist() 

        # 1. Ù…Ù†Ø·Ù‚ ØªØ¹ÛŒÛŒÙ† Ø³ØªÙˆÙ† Ù‡Ø¯Ù
        if selection_mode == "user":
            # Ø­Ø§Ù„Øª Ú©Ø§Ø±Ø¨Ø±: Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
            if not target_column or target_column not in all_columns:
                return JSONResponse(status_code=400, content={
                    "message": "Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ú©Ø§Ø±Ø¨Ø±ØŒ Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ù…Ø¹ØªØ¨Ø± Ùˆ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø§ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆØ¯.",
                    "all_columns": all_columns,
                    "target_column": target_column if target_column else "N/A",
                    "user_id": user_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                    "room_id": room_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                    "initial_status": "Error"
                })
            target_to_use = target_column
            
        else: # selection_mode == "ai"
            # ğŸ’¥ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ AI
            ai_suggestions = await get_ai_suggestions_from_deepseek(df, ALLOWED_REGRESSORS)
            
            target_to_use = ai_suggestions.get("target_column")
            algo_to_use = ai_suggestions.get("algorithm", algo_to_use)
            
            if target_to_use is None or target_to_use not in all_columns: 
                 # Ø§Ú¯Ø± AI Ù†ØªÙˆØ§Ù†Ø³Øª Ø³ØªÙˆÙ† Ù…Ø¹ØªØ¨Ø±ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯
                return JSONResponse(content={
                     "message": "AI Ù†ØªÙˆØ§Ù†Ø³Øª Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ù…Ø¹ØªØ¨Ø±ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯. Ù„Ø·ÙØ§Ù‹ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ù„ÛŒØ³Øª Ø²ÛŒØ± ÛŒÚ© Ø³ØªÙˆÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.",
                     "all_columns": all_columns,
                     "target_column": "N/A",
                     "user_id": user_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                     "room_id": room_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                     "initial_status": "ColumnSelectionRequired"
                 })

        # 2. Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² to_thread Ø¨Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ CPU-Bound)
        prediction_results = await to_thread.run_sync(
            run_prediction, 
            df, target_to_use, algo_to_use, input_filename, config_model.test_size, config_model.random_state
        )

        # 3. ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        final_response = {
            "message": f"ØªØ­Ù„ÛŒÙ„ Ù…Ø§Ù„ÛŒ Ø¨Ø§ Ø³ØªÙˆÙ† '{target_to_use}' Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.",
            "target_column": prediction_results.get("target_used", "N/A"),
            "algorithm_used": prediction_results.get("algorithm_used", "N/A"),
            "prediction_plot_data": prediction_results.get("prediction_plot_data", {}),
            "metrics": prediction_results.get("metrics", {}),
            "all_columns": all_columns,
            "user_id": user_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
            "room_id": room_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
            "initial_status": "Success" 
        }
        
    except HTTPException as e:
        raise e
    except Exception as e:
        # Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ - Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ all_columns
        error_message = f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„: {type(e).__name__}: {str(e)}"
        return JSONResponse(
            status_code=500, 
            content={
                "message": error_message,
                "all_columns": all_columns,
                "target_column": target_to_use,
                "user_id": user_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                "room_id": room_id,  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                "initial_status": "Error"
            }
        )

    return JSONResponse(content=final_response)